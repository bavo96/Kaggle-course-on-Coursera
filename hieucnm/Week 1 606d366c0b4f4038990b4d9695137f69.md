# Week 1

Created By: Hiếu Cao Nguyễn Minh
Last Edited: Aug 2, 2020 11:51 AM
Tags: data science
Tóm tắt: Note some significant ideas from this course

# *Week 1 - Overview*

## *1. Competitions Platforms*

Những trang tổ chức thi data science như Kaggle: 

1. [*Driven Data*](https://www.drivendata.org/)
2. *[CodaLab](https://codalab.org/)*
3. *[IDAO](https://idao.world/)*
4. *[TopCoder](https://www.topcoder.com/challenges?filter[tags][0]=Data%20Science&filter[tracks][data_science]=true&tab=details)*
5. *[DataHack](https://datahack.analyticsvidhya.com/contest/all/)*
6. *[Tianchi](https://tianchi.aliyun.com/competition/gameList/activeList)*

## 2. *Real world ML Pipeline*

1. Understanding business problem
2. Problem formalization
3. Data collecting
4. Data preprocessing
5. Modelling
6. Deploy model
7. Monitoring
- Kaggle only focus on step 4,5 and sometime 3 of the pipeline.

## *3. Advices*

1. **Not only model:** Để win Kaggle nói riêng và data science nói chung, thì mấu chốt ko nằm ở model, mà ở việc chúng ta hiểu data như thế nào. Thay vì cứ loay hoay improve model một cách heuristic, hãy tìm insight bên trong data để đưa ra quyết định nên improve cái gì, và improve như nào cho đúng.
2. **Be creative:** suy nghĩ trên nhiều phương diện, đừng nên nghĩ rằng cứ model mới thắng, đôi khi chỉ cần phân tích ra cái rule gì đó và apply là vẫn thắng, đôi khi chả cần ML vẫn thắng, đừng ngại dùng thuật toán phức tạp, tính toán lâu, vì Kaggle lo chuyện hardware bên dưới cho mình rồi. Luôn think out of the box.

## *4. Recap of ML algos*

1. Linear (include kernel-based like SVM)
2. kNN
3. Tree-based (most powerful for tabular data)
4. Neural Net (most powerful for unstructure data: images, text, ...)
- ***No free lunch :*** không thuật toán nào là da best cho mọi bài toán.
- Hình dạng của Decision boundary của các thuật toán khác nhau (phải thuộc nằm lòng):
    1. Linear: split feature space into subspaces, boundaries are lines
    2. Tree-based: split feature space into boxes
    3. kNN: no split, depends heavily on which distance metric to use
    4. Neural Net: same as linear model but smoothly, boundaries are curves

![Week%201%20606d366c0b4f4038990b4d9695137f69/Untitled.png](Week%201%20606d366c0b4f4038990b4d9695137f69/Untitled.png)

Source : sklearn plot classifier comparision

## *5. Quiz about GDBT*

- Will GDBT perform worse if we remove first/last tree ?

    Removing last tree will not hurt the performace.

    Removing first tree depends on learning rate: If learning rate is too large, performace will drop significantly, and vice versa.

## *6. Feature preprocessing: Numeric*

- Scaling:
    - Not necessary for tree-based models.
    - With respect to non-tree-based models, we should apply scaling (MinMaxScaling, StandardScaling, ...) to ALL numeric features, so that all features will have roughly similar initial impact on model, prevent model from strongly depending on only some extreme features.
- Outlier handling:
    - Clip: numeric features (and also numeric target) by a upper bound (usually 99th percentile) and lower bound (usually 1st percentile), so that the feature distribution look fine.
    - Rank: will pull outliers closer to normal sample.
- Feature generation:
    - Log transform
    - Power transform
    - Get numbers after floating point, e.g. 3.92 → 0.92
    - ...

## *7. Feature preprocessing: Categorical*

- Label Encoding:
    - work fine if the categorical feature contains ordinal meaning
    - we can encode by multiple orders: alphabet order, appearance order, ...
    - often used for tree-based models
- Count / Frequency Encoding:
    - work fine if the frequency of the categorical feature is correlated to the target
    - if it is, then this also help reduce number of split of tree-based models.
    - often used for tree-based models
    - can be applied on only train data or both train and test data (i.e: count the values on both train and test data to encode).
- One-hot encoding:
    - already scaling
    - hurt memory and make training/inference time slower if there are too many unique values of the categorical feature. We can overcome this by sparse matrix.
    - often used for non-tree-based models.
- Feature generation:
    - Combine multiple categorical features to form new one (e.g combine feature A (2 unique values) and B (3 unique values) to a new feature C (2*3 = 6 unique values), useful for non-tree based models.

## *8. Feature preprocessing: Datetime*

There are different ways to generate feature from a datetime column:

- Time period: year, month, day-of-month, day-of-week, week-of-month, hour, minute, second, season, is_weekend, is_holiday, ...
- Time since: calculate a time range between current time to a specified time. Time range can be in seconds, days, or any period unit. There are 2 sub-types of this:
    - Row-independent: the specified time is the same for all sample. E.g: timestamp is calculated from current time to 00:00:00 UTC, 1st Jan 1970.
    - Row-dependent: the specified time is different for each sample. E.g: number of days left until next holidays, ...
- Time diff: difference between datetime columns.

![Week%201%20606d366c0b4f4038990b4d9695137f69/Untitled%201.png](Week%201%20606d366c0b4f4038990b4d9695137f69/Untitled%201.png)

Source: Coursera. An example of feature generation for datetime.

After generated, new features can be either numeric or categorical features. Depending on which kind they are, we should apply suitable preprocessing steps in the above section.

## 9*. Feature preprocessing: Coordinate*

Feature generation:

- Distance: find one or some specific points in our map and calcualte distance of each sample to those points. The specific points depend on each task, and we also have multiple ways to choose them.
    - in case we have a real map: the specific points can be hospitals, malls, or gates, ...
    - in case we dont have a map in our data, we can use coordinates in our whole training data to build a grid map, then use some techniques to find specific points:
        - we can use a clustering algorithm and consider the desired centers as those specific points.
        - or we can leverage statistics on other columns. E.g: we can use areas where there are many old buildings around, we can obtain this information if we have something like building_age column in our data; or we can use area having the highest average building prices (or population ...), we can obtain this infor if we have something line building_price coumn in our data; and so on ...
- If we are going to use tree-based models, one tricky for coordiate columns is that we can rotate the coordinates to have new features. The rotation degree cannot be known in advance, just try and error. This trick can work because tree-based models split data into boxes, number of splits can be reduced if features stand in the right place. Look at the example image below to clearly understand:

![Week%201%20606d366c0b4f4038990b4d9695137f69/Untitled%202.png](Week%201%20606d366c0b4f4038990b4d9695137f69/Untitled%202.png)

Source: Coursera. Number of splits could be only 1 if we rotate coordinate features in a right degree.

## 10*. Missing values*

- Missing values can be in form of NaN in our data, or the NaNs can be replaced in form of a specifial values that occur too many times, such as: empty string, -1, -999, a very large number, ... The most way to find out hidden NaNs in a column is:
    - with numeric: checking the historgam.
    - with categorical: checking the bar plot of value counts

- Ways to fillna:
    - *-1, -999, "miss"* : replace NaNs by a value outside of value range, or consider missing values as a seperate category. This method can be usefull for tree-based models, but not good for non-tree-based models.
    - *mean, median*: replace NaNs by mean or median of that non-missing values of that numeric column. Contrary, this method is beneficial for non-tree-based models, such as linear and neural nets, and not good for tree-based models.
    - reconstruct missing values somehow (check it later)
- Feature generation:
    - *f0_is_null* feature: a new feature indicating that a value of column *"f0"* is missing or not. **This is very useful for any models**.

- **Avoid feature generation after missing values imputation.** Because the imputed values (e.g: mean, -999, ...) can mislead our model. Let's see an example.
    - We have 2 columns: temperature  and date. Column temperature has NaN values. Now, we fill NaN values of temperature by mean.  Then, we generate a new feature that is the different temperature between 2 adjacent days.
    - The problem here: In natural sense, average temperature of a whole year will fall back into approximately zero. Therefore, missing values was imputed by zeros. So, our new feature will become abnormally large at rows where temperature of one of the two adjacent days is zero. This will hurt the learning process of our model. See the figure below to clearly understand.

    ![Week%201%20606d366c0b4f4038990b4d9695137f69/Untitled%203.png](Week%201%20606d366c0b4f4038990b4d9695137f69/Untitled%203.png)

    Source: Coursera. Feature generation after NaN imputation can create wrong features.

- Some more trick for handling miss values:
    - Consider outliers as missing values.
    - Consider values that appear in train data but do not in test data as missing ones. This is a good approach because it forces our model to learn other aspects whenever it see those not-in-test-data values.
        - But if we tend to use count / frequency encoding for this column on both train and test data, we should not apply this trick. Because, if there is a relation between frequencies of value in this column with the target column, then removing the values can drop useful information for our model. See the example below:

![Week%201%20606d366c0b4f4038990b4d9695137f69/Untitled%204.png](Week%201%20606d366c0b4f4038990b4d9695137f69/Untitled%204.png)

Source: Coursera. Example that removing not-in-test values can drop useful relation between frequency feature and target column.

## 11*. Feature extraction: text*

Approaches to extract feature from text:

- Bag of words: very large vectors, no meaning captured.
- Tf-Idf (old but gold): still very large vectors, captured importance of words in a document.

$$tf(t,d) = \frac{count(t,d)}{\max \{ count(w,d) : w \in d \}}$$

$$idf(t, D) = log \frac{ |D| }{| \{d \in D : t \in d \}|}$$

- Word embedding: small vectors, captured meaning of words, words having relationship (e.g: synonyms) will have corresponding vectors that are close (in term of euclidean / cosine distance) to each other.

Approaches to preprocess text data:

- Lowercase
- Leematization: turn plural nouns to singular nouns, verbs to base form.
- Stemming : remove different characters in the tail of similar words.
- Stopwords removal: remove words appear too many times.